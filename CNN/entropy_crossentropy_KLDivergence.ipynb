{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Entropy, Cross-entropy, KLDivergence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Entropy:\n",
    "\n",
    "*   Entropy là một khái niệm được sử dụng trong lý thuyết thông tin và machine learning để đo lường độ không chắc chắn của một tập dữ liệu. Entropy được tính bằng công thức:\n",
    "\n",
    "$$\n",
    "H(X) = -∑ P(x) log₂ P(x)\n",
    "$$\n",
    "\n",
    "\n",
    "*   Trong đó, X là biến ngẫu nhiên và P(x) là xác suất của biến đó.\n",
    "\n",
    "*   Entropy có thể được hiểu như độ lộn xộn, độ không chắc chắn của một tập dữ liệu. Nếu một tập dữ liệu có entropy cao, điều đó có nghĩa là tập dữ liệu đó có nhiều đa dạng và không chắc chắn. Nếu một tập dữ liệu có entropy thấp, điều đó có nghĩa là tập dữ liệu đó ít đa dạng và có ít độ không chắc chắn hơn.\n",
    "\n",
    "*   Trong machine learning, entropy thường được sử dụng để tính toán thông tin lượng mà một thuật toán phân loại cần để phân loại các mẫu dữ liệu. Khi xây dựng một cây quyết định, entropy được sử dụng để tính toán độ chính xác của các phân loại khác nhau và chọn phân loại tốt nhất cho mỗi nút của cây. Các thuật toán phân loại khác cũng sử dụng entropy để đo lường độ chính xác của các phân loại và tối ưu hóa các thuộc tính để đạt được hiệu quả cao nhất trong việc phân loại.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ['Red', 'Green', 'Blue']\n",
    "\n",
    "sample1 = [200, 450, 250] # -> 200/(200+450+250) -> Tỷ lệ màu đỏ trong toàn bộ hỗn hợp\n",
    "sample2 = [310, 250, 448]\n",
    "\n",
    "p = [0.333, 0.333333, 0.333333335] # được hiểu là xác suất, ở đây chỉ bằng nhau về xác suất\n",
    "p1 = [1, 0.0000000001, 0.0000000001] # đồng nhất về vật liệu, chỉ một trường hợp nào đó xảy ra và các trường hợp khác không xảy ra\n",
    "q = [0.31, 0.25, 0.44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0985792184763945\n"
     ]
    }
   ],
   "source": [
    "entropy_p = -np.sum([p[i] * np.log(p[i]) for i in range(len(p))])\n",
    "print(entropy_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.605170185988092e-09\n"
     ]
    }
   ],
   "source": [
    "entropy_p1 = -np.sum([p1[i] * np.log(p1[i]) for i in range(len(p1))])\n",
    "print(entropy_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.070871757456611\n"
     ]
    }
   ],
   "source": [
    "entropy_q = -np.sum([q[i] * np.log(q[i]) for i in range(len(q))])\n",
    "print(entropy_q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cross-entropy:\n",
    "\n",
    "*   Cross-entropy là một hàm mất mát thường được sử dụng trong các mô hình học máy, bao gồm cả mạng nơ-ron tích chập (CNN). Trong CNN, cross-entropy thường được sử dụng như một hàm mất mát để đánh giá hiệu suất của mô hình trong quá trình huấn luyện.\n",
    "\n",
    "*   Để hiểu rõ hơn về cross-entropy, trước tiên cần hiểu về khái niệm \"entropy\". Entropy được sử dụng để đo độ \"bất định\" của một phân bố xác suất. Cross-entropy là một đại lượng được tính toán dựa trên hai phân bố xác suất khác nhau, thường được sử dụng để đánh giá sự khác biệt giữa phân bố xác suất thực tế của dữ liệu và phân bố xác suất được dự đoán bởi mô hình.\n",
    "\n",
    "*   Trong CNN, cross-entropy thường được sử dụng để tính toán mất mát cho bài toán phân loại đa lớp. Cụ thể, giả sử chúng ta có một mô hình CNN để phân loại hình ảnh vào một trong các lớp khác nhau, và chúng ta có một tập dữ liệu huấn luyện với các hình ảnh được gán nhãn đúng lớp tương ứng. Cross-entropy được sử dụng để đánh giá sự khác biệt giữa phân bố xác suất thực tế của các lớp và phân bố xác suất được dự đoán bởi mô hình. Cụ thể, cross-entropy được tính bằng cách lấy tổng của các giá trị đóng góp của từng mẫu dữ liệu vào mất mát.\n",
    "\n",
    "*   Tóm lại, cross-entropy là một hàm mất mát thường được sử dụng trong CNN để đánh giá hiệu suất của mô hình trong quá trình huấn luyện. Nó đo độ khác biệt giữa phân bố xác suất thực tế và phân bố xác suất được dự đoán bởi mô hình, và thường được sử dụng trong các bài toán phân loại đa lớp."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Công thức toán học của cross-entropy được tính bằng cách lấy tổng của các giá trị đóng góp của từng mẫu dữ liệu vào mất mát. Giả sử chúng ta có một tập dữ liệu huấn luyện với m mẫu dữ liệu và k lớp khác nhau. Mỗi mẫu dữ liệu được biểu diễn dưới dạng một vector đầu vào x, và được gán nhãn đúng là một vector đầu ra y, trong đó chỉ có một phần tử của y bằng 1 và phần còn lại bằng 0 (tức là mỗi mẫu dữ liệu chỉ thuộc về một lớp).\n",
    "\n",
    "*   Công thức toán học của cross-entropy cho một mẫu dữ liệu (x, y) được tính bằng:\n",
    "\n",
    "$$\n",
    "H(y, \\hat{y}) = - \\sum_{i=1}^{k} y_i log(\\hat{y_i})\n",
    "$$\n",
    "\n",
    "*   trong đó:\n",
    "\n",
    "    $y_i$ là phần tử thứ i của vector đầu ra y (tức là đúng lớp của mẫu dữ liệu).\n",
    "    $\\hat{y_i}$ là phần tử thứ i của vector đầu ra dự đoán bởi mô hình cho mẫu dữ liệu x.\n",
    "    $log(\\hat{y_i})$ là logarit tự nhiên của phần tử thứ i của vector đầu ra dự đoán.\n",
    "    Công thức trên tính toán mức độ khác biệt giữa phân bố xác suất thực tế của các lớp và phân bố xác suất được dự đoán bởi mô hình. Mất mát của một tập dữ liệu huấn luyện được tính bằng cách lấy trung bình của cross-entropy trên toàn bộ các mẫu dữ liệu:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{m} \\sum_{i=1}^{m} H(y^{(i)}, \\hat{y}^{(i)})\n",
    "$$\n",
    "\n",
    "*   trong đó:\n",
    "\n",
    "    $y^{(i)}$ và $\\hat{y}^{(i)}$ lần lượt là vector đầu ra thực tế và vector đầu ra dự đoán bởi mô hình cho mẫu dữ liệu thứ i. Mục tiêu của quá trình huấn luyện là tối thiểu hoá mất mát L này bằng cách điều chỉnh các tham số của mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1257617765072347\n"
     ]
    }
   ],
   "source": [
    "# lấy p tham chiếu làm gốc\n",
    "cross_entropy_pq = -np.sum([p[i] * np.log(q[i]) for i in range(len(p))])\n",
    "print(cross_entropy_pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0989226915716455\n"
     ]
    }
   ],
   "source": [
    "# lấy q tham chiếu làm gốc\n",
    "cross_entropy_qp =  -np.sum([q[i] * np.log(p[i]) for i in range(len(p))])\n",
    "print(cross_entropy_qp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     KLDivergence\n",
    "\n",
    "*   KLDivergence (Kullback-Leibler Divergence) là một độ đo khoảng cách giữa hai phân bố xác suất. Được đặt tên theo hai nhà toán học Solomon Kullback và Richard Leibler, độ đo này thường được sử dụng trong lĩnh vực xử lý ngôn ngữ tự nhiên, học máy và thị giác máy tính để so sánh hai phân bố xác suất khác nhau, ví dụ như phân bố xác suất của một mô hình và phân bố xác suất thực tế.\n",
    "\n",
    "*   KLDivergence được định nghĩa như sau: Cho hai phân bố xác suất P(x) và Q(x), KLDivergence của P(x) so với Q(x) được tính bằng:\n",
    "\n",
    "$$\n",
    "D_{KL}(P||Q) = \\sum_{x \\in X} P(x) log \\frac{P(x)}{Q(x)}\n",
    "$$\n",
    "\n",
    "*   trong đó:\n",
    "\n",
    "    P(x) là phân bố xác suất thực tế, Q(x) là phân bố xác suất được dự đoán bởi mô hình hoặc ước lượng từ dữ liệu và X là không gian xác suất.\n",
    "\n",
    "    Giá trị KLDivergence có thể được hiểu như khoảng cách giữa hai phân bố xác suất, nó không phải là một khoảng cách đối xứng, có nghĩa là $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$.\n",
    "\n",
    "    Giá trị KLDivergence luôn là không âm, và nó đạt giá trị 0 nếu và chỉ nếu hai phân bố P(x) và Q(x) giống nhau về mặt xác suất (tức là P(x) = Q(x) với mọi giá trị x). Do đó, khi sử dụng KLDivergence để đánh giá hiệu quả của mô hình, chúng ta cần cân nhắc đến giá trị này để đưa ra những đánh giá chính xác."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027182558030840298 0.02805093411503462\n"
     ]
    }
   ],
   "source": [
    "klDivergence_pq = np.sum([p[i] * np.log(p[i]/q[i]) for i in range(len(p))])\n",
    "klDivergence_qp = np.sum([q[i] * np.log(q[i]/p[i]) for i in range(len(q))])\n",
    "print(klDivergence_pq, klDivergence_qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0989226915716455\n",
      "1.0989226915716455\n"
     ]
    }
   ],
   "source": [
    "print(cross_entropy_qp)\n",
    "print(entropy_q + klDivergence_qp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
